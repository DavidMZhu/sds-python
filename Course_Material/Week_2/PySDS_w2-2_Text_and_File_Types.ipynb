{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 2. Lecture 1. V.1** Author: Bernie Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2. Day 2. Text processing and file input output. \n",
    "=====================================================\n",
    "\n",
    "Please note, that today you will want to work with some files that have been presented for you in the file (under supplementary files), ```PySDS_Week02_Day02_fileWork.zip```.  \n",
    "\n",
    "Unzip this file into the same directory as your ipynb files unless you want to change the file paths in the examples below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Introduction to File types \n",
    "\n",
    "For each of the following file types, we will have a bit of a summary of the file type, it's relevance and it's structure. Then we will discuss about how to get the file type into and out of a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON \n",
    "\n",
    "JavaScript Object Notation [JSON] is one of the most popular forms for data transport. This is partially because it is easy in javascript to export data to this format. It is also partialy because it is very lightweight. By using only brackets, quotes and braces, you can structure most of the data. \n",
    "\n",
    "Javascript looks very similar to python lists and dictionaries. In fact, I'm at a loss to think of a difference off hand. That being said, it is not advised to build your own json parser when python has one built in. \n",
    "\n",
    "As json is just lists and dicts it stands to reason that it could be human readable. If you will notice from looking at json on the web, it does not usually include whitespace. Yet, programs can often 'pretty print' the whitespace or better, turn the json automatically into a hierarchical structure to browse. On different browsers, try going to:\n",
    "~~~\n",
    "https://www.reddit.com/.json \n",
    "~~~\n",
    "All of reddit's pages can be formatted as json. This makes them available to third party app providers. That being said, only some pages are accessible. Without proper authentication, you cannot view a user's private account details. But otherwise, by placing .json at the end of a reddit page and before the arguments, you can access it for later analysis.  \n",
    "\n",
    "You can load json data with: \n",
    "~~~\n",
    "import json\n",
    "\n",
    "datastructure = json.loads(<The_Data>)\n",
    "~~~\n",
    "\n",
    "Then you can just query it as a series of nested lists and dictionaries. You can also print this data in a nice readable format (called pretty printing) in the following way: \n",
    "\n",
    "~~~\n",
    "json.dumps(THE_DATA, indent=4) \n",
    "~~~\n",
    "\n",
    "There are other ways to pretty print. Let's look at json from Reddit pretty printed. Open a browser window and head to:\n",
    "\n",
    "https://jsonformatter.curiousconcept.com \n",
    "\n",
    "In there type: \n",
    "\n",
    "http://reddit.com/r/aww.json \n",
    "\n",
    "Notice how you can collapse and expand the json file. We will use this to navigate through the file then download it for processing. \n",
    "Json needs to be imported otherwise python will not recognize the commands. In python 2.* json was not fully implemented and people often had to go to the django package for a robust json parser. In python 3 we typically use the base package (```import json```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in json \n",
    "import json\n",
    "\n",
    "filein = json.loads(open(\"muppetEpisodes.json\").read())\n",
    "print(type(filein))\n",
    "print(len(filein))\n",
    "print(len(filein['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping data to json. \n",
    "\n",
    "fileout = open(\"muppetEpisodes_prettyPrinted.json\",'w')\n",
    "# fileout.write(\n",
    "print(json.dumps(filein,indent=4)) \n",
    "# fileout.close()\n",
    "\n",
    "# Now let's open that file in a text editor to see what's up. \n",
    "# Also try opening it in your browser! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My favorite part: json.normalize\n",
    "\n",
    "# NOTE: There is a bug in one version of PANDAS. If this code doesn't work for you\n",
    "#       a. Update Pandas\n",
    "#       b. SKip it. \n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "filein = json.loads(open(\"muppetEpisodes.json\").read())\n",
    "muppet_table = json_normalize(filein[\"data\"])\n",
    "display(muppet_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in muppet_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "HyperText Markup Language, or a means to mark up text in such a way that it can be rendered by a browser. HTML is only a tiny slice of the web at the moment. A huge amount is done outside HTML with the use of CSS (cascading style sheets) and JavaScript. These allow pages to be more dynamic. \n",
    "\n",
    "Both XML and HTML are examples of mark-up **languages**. That is the documents follow a hierarchical structure and use mark-up tags to indicate which part of the structure you are in. The tags that denote the sturcture are in brackets. Open tags have a word (and some options), closing tags have the same word but with a / in front of it. Here is a basic HTML document\n",
    "\n",
    "    <html>\n",
    "        <head>\n",
    "            <title> \n",
    "                This is the title! \n",
    "            </title>\n",
    "        </head>\n",
    "        <body>\n",
    "            This is a webpage!\n",
    "        </body>\n",
    "    </html>\n",
    "\n",
    "We won't spend too much time here on HTML, as it should be familiar, at least in its basic format. Some important things to remember: \n",
    "\n",
    "- All HTML pages start and end with ```<html>```  ```</html>```.\n",
    "- Pages should have a title and a body. The title contains the header and the body contains most of the rest of what is rendered. There are conventions for things like where to put javascript code. \n",
    "- HTML has an element tree. The root node is ```<html>```, the child nodes are ```<body>``` and ```<title>``` among others. This hierarchical structure allows code to know where to render text. \n",
    "- Some HTML only exists on the browser, as it was rendered by a pre-hypertext processor (or PHP). PHP along with SQL are the harbingers of Web 2.0. They allowed pages to be created on the fly and bespoke rather than generic. Before PHP we had geocities, afterwards we have MySpace and Facebook. I'll let you decide whether that was progress ;) \n",
    "\n",
    "We will most likely want to parse HTML pages. This will happen next week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML\n",
    "\n",
    "XML stands for 'extensible mark-up language'. XML files can be generic or have a document type. For exmaple, GraphML is really just XML with a specific schema that is used for social network graph types. \n",
    "\n",
    "Like HTML, XML is a markup language with less than and greater than to encase the element tags. The text inside these tags must have some special characters escaped. \n",
    "\n",
    "~~~ xml \n",
    "<start> \n",
    "    <middle>\n",
    "        <end1>   Here is an element! </end1>\n",
    "        <end2>   Here is an element! </end2>\n",
    "    </middle>\n",
    "</start>\n",
    "~~~\n",
    "\n",
    "Elements have an \"element tree\". Above, ```start``` is the root node, ```middle``` is a child and ```end1``` is a child of middle. ```end1``` and ```end2``` are siblings. \n",
    "\n",
    "XML is a self-documenting style, which means that you can insert details about the elements into the document itself. This can be accomplished with keys that are often prepended to the top of the document just below any details about the formatting. \n",
    "\n",
    "Most of the time, we will not be so concerned with the top of an XML document. Rather, we will simply want to navigate the element tree to get to the element(s) that are of concern to us. \n",
    "\n",
    "In the script below, we will use load in XML as a string. Then we will use a module called 'beautiful soup' to navigate the document and return aspects of the XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading some xml\n",
    "import bs4 \n",
    "infile = open(\"Canada.xml\",'r')\n",
    "\n",
    "wikitext = infile.read()\n",
    "\n",
    "# Note: Depending on how the file comes down it might be encoded, in which case\n",
    "# use the .decode('utf-8') function on the text. \n",
    "# soup = bs4.BeautifulSoup(wikitext.decode('utf8'), \"lxml\")\n",
    "soup = bs4.BeautifulSoup(wikitext, \"lxml\")\n",
    "\n",
    "print (soup.mediawiki.page.revision.id )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating XML\n",
    "\n",
    "Navigating XML involves moving up and down or sideways along the element tree. In the case above it was clear that I know where to go for the text I wanted. In general, however, navigating to the right element is a bit fiddly. It is especially challenging when there are sibling nodes. However, beautifulsoup normally does a reasonable job of helping out with this task. Below you can watch me navigate through the tags. Notice that even though mediawiki is actually at: \n",
    "~~~ html\n",
    "<html>\n",
    "    <body>\n",
    "        <mediawiki>\n",
    "            ...\n",
    "~~~ \n",
    "\n",
    "We do not need the full path to access it. BeautifulSoup will return ```mediawiki``` by going to ```soup.mediawiki.text```. But also note, that this is not the page's text, but the text under media wiki which includes some other text. To get the page for parsing, we would go to mediawiki.page.text. Yup, it's tedious. \n",
    "\n",
    "The **good part** is that if you _already_ know what you are looking for by element you can ask for it directly. If there are multiples of the same thing (such as 'a' for a link in an html page), you can just query ```soup.a``` and it will iterate through all of them with that tag without going ```soup.html.body.a``` or finding out where it is located in the element tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = \"\\n\" + \"~\"*20 + \"\\n\"\n",
    "\n",
    "for i in soup.children: print(i.name)\n",
    "print(end)\n",
    "\n",
    "for i in soup.html.children: print(i.name)\n",
    "print(end)\n",
    "\n",
    "for i in soup.html.body.children: print(i.name)\n",
    "print(end)\n",
    "\n",
    "for i in soup.mediawiki.children: print(i.name)\n",
    "print(end)\n",
    "\n",
    "for i in soup.mediawiki.page.children: print(i.name)\n",
    "print(end)\n",
    "    \n",
    "y = soup.page.text\n",
    "\n",
    "print(y[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bytestreams \n",
    "\n",
    "Sometimes we will want to use bytestreams in order to read in data. It is not very common, but for example when reading in zipfiles by code, it is important. Below is an example of reading in a file and then as a bytestream to see the difference. \n",
    "\n",
    "Bytestreams are **encoded** character sets. (i.e. they are written in the code computers understand). Strings have been **decoded** so that they can be printed for people to read. Depending on your operating system, you might not be able to write a file in the correct encoding by default. Windows can be a bit more tricksy than *Nix systems in this regard. \n",
    "\n",
    "We typically want to decode to UTF-8 which will write the file with the code points that a computer can use to decode the file when it needs to represent the character to a user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"hello 👋\"\n",
    "\n",
    "try: \n",
    "    fileout = open(\"temp.txt\",'w')\n",
    "    fileout.write(x)\n",
    "    fileout.close()\n",
    "except UnicodeEncodeError: \n",
    "    print(\"This program may have difficulty encoding the emoji\")\n",
    "\n",
    "fileout = open(\"temp.txt\",'w',  encoding='UTF-8')\n",
    "fileout.write(x)\n",
    "fileout.close()\n",
    "\n",
    "print(\"Below we are reading as a string a file that has been encoded\")\n",
    "filein = open(\"temp.txt\",'r')\n",
    "print(filein.read())\n",
    "filein.close()\n",
    "\n",
    "print(\"Below we are reading as a byte and then decoding it\")\n",
    "filein = open(\"temp.txt\",'rb')\n",
    "print(filein.read().decode('UTF-8'))\n",
    "filein.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see: \n",
    "\n",
    "```\n",
    "hello ðŸ‘‹\n",
    "```\n",
    "Followed by \n",
    "```\n",
    "hello 👋\n",
    "```\n",
    "Or two waving hands? On Windows I suspect you saw the above and on \\*Nix, it was two emoji hands.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Sometimes, you want to close a program and pick up right where you left off. This might mean ensuring that all the objects are in the state that you want them to be with no further processing. This process of creating a file that will represent the state of some values is called serialization. We 'serialize' variables or data structures. \n",
    "\n",
    "Now, python being python, they had to give it a more friendly name. So in python if you want to save the state of a variable or set of variables as is, you can 'pickle' them. You can then 'unpickle' them later on. \n",
    "\n",
    "One useful approach with pickling is when you are processing text on a server and you are doing something complicated, you can pickle all your current state of things if the program goes sour, then pick up where you left off.\n",
    "\n",
    "You can only serialize one object at a time, but of course that object can be a collection of numerable other objects. Since these files are meant for the computer, they might not make complete sense read as text. But we can save them and read them into a new variable later. This is done with the following syntax: \n",
    "\n",
    "~~~ python\n",
    "import pickle \n",
    "x = <object> \n",
    "pickle.dump(x,open(<file>,'wb')) \n",
    "~~~\n",
    "\n",
    "And to load the object again (with any name):\n",
    "~~~ python\n",
    "y = pickle.load(open(<file>), 'rb')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "x = ['1','2']\n",
    "pickle.dump(x,open(\"temp.txt\",'wb'))\n",
    "y = pickle.load(open(\"temp.txt\",'rb'))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickles can expire: Check the version number\n",
    "\n",
    "Notice that we are using 'rb' and 'wb' with the pickles. This is because Python 3's default pickling version writes the pickled object as a bytestream. Also note that this pickled object will probably not be readable by python 2.  You can set the pickle protocol so that it is readable by python 2 as an option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV \n",
    "\n",
    "Comma-separated values is a common data storage format. Yet, despite it's prevalence, there are a few variations to consider:\n",
    "- How are strings represented? Do they use \"<string>\" for every string, no string or only strings with commas in them? \n",
    "- How are new lines represented? \n",
    "- Is there a header? \n",
    "- Is there a trailing \\n?\n",
    "\n",
    "It is simple to roll your own csv reader. So much so that you did a version of this on the first day. Yet, there are often enough details to consider that you might want to rely on an external program. Python will offer two. First is the ```csv``` module. This is a standalone package that can be imported. It has many options for separators and whether there's a header. It also has some nice ways to index the data. For example, if you want to store your data as a dictionary with the header as the key and the column as the values, this does the trick. \n",
    "\n",
    "The basic usage, however, is to iterate through a file line by line. Instead of iterating through with 'readline' and splitting the text that comes back, you create a \"csv reader\", and this iterates line by line returning not a string of text, but a list split at every comma (or user-defined separator). \n",
    "\n",
    "~~~ python \n",
    "import csv \n",
    "\n",
    "with open('data.csv', newline='') as file_to_read:\n",
    "    filereader = csv.reader(file_to_read, delimiter=' ', quotechar='|')\n",
    "    for row in filereader:\n",
    "        print('<>'.join(row))\n",
    "~~~\n",
    "\n",
    "The nice thing about ```csv```, particularly when not using pandas, is the use of the DictReader. This returns a dictionary with the header as the key and the values in that row as the value. If there's no header line, you can specify a list as ```fieldnames```. \n",
    "\n",
    "The second is simply reading in using pandas default importer. \n",
    "\n",
    "~~~\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(<path_to_file>) \n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel \n",
    "\n",
    "Excel is the popular spreadsheet program from Microsoft. Files can be stored as either .xls or .xlsx. The first one is a bytestream proprietary file format, but the details are handled by PANDAS. The second one was published as an open standard and is in fact a wrapper over a specific format of xml.  \n",
    "\n",
    "In general, we simply want to import a sheet with:\n",
    "\n",
    "~~~ python \n",
    "<sheet> = pd.read_excel(<file_path>) \n",
    "~~~\n",
    "\n",
    "However, I strongly encourage you to check with the documentation. See: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\n",
    "You can also see in this document the trials with trying to remember function names. They've deprecated the 'sheetname' argument, to use 'sheet_name' whereas 'skip_footer' has been deprecated to use 'skipfooter'. Frustrating, I know. \n",
    "\n",
    "Excel documents can be pretty complex. Pandas will read the first sheet, but not the others unless specified. It also is not great with headers as can be seen below. Your mileage may vary, most important is to be mindful of the data, **check it before you use it**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "mt = pd.read_excel(\"MuppetsTable.xlsx\")\n",
    "display(mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Basic Text Scraping\n",
    "\n",
    "Basic text scraping is the practice of taking some data and cleaning it in such a way that it can be used for other programs. Below are a series of excercises designed to help you understand the fundamentals of text processing. In particular, we will focus on the process of handling whitespace. This will involve using several additional files that should be uploaded to your workspace. \n",
    "\n",
    "1. Cleaning up by line breaks. \n",
    "2. Splitting text by space. \n",
    "3. Finding specific words and characters. \n",
    "4. Converting from one character set to another character sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.1 - Stripping characters.**\n",
    "\n",
    "Below we will take a file, read it, print it and then get rid of the return characters. Please pay attention to the line-breaks when the file is being printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning up the lines**\n",
    "Did you notice that each of the lines has a space in between them? This is because we printed:\n",
    "\n",
    "    \"Testing Line 1\\n\"\n",
    "    \n",
    "And this is becuase when python reads the file it does it line by line. It splits the file at the new line character but keeps that character in the string when it returns the string. To get rid of these new line characters we would **strip()** the whitespace from the ends of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        print(i.strip()) #strip() removes whitespace from left and right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "words = []\n",
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        words += i.split() #by default, split uses a single space character to split. \n",
    "                           # Simply put a different separator in as argument to split by something else \n",
    "        \n",
    "wordseries = pd.Series(words)\n",
    "\n",
    "print(wordseries.value_counts())\n",
    "print() \n",
    "\n",
    "# Here we can print only the words with a count greater than 1\n",
    "for i in list(wordseries.value_counts()[wordseries.value_counts() > 1].index):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word frequency**\n",
    "\n",
    "So, as we can see above, we have all sorts of issues with words. The word 'line' is there in upper and lower case, sometimes the text uses numbers, sometimes it has periods in there. We can do all sorts of things to these data to  clean them. \n",
    "\n",
    "Here is a simple script that first puts all the words into lower case then chops off the left and right side of they are punctuation. This is an illustrative script. In practice, you will want to consider the ```nltk``` library (standing for natural language toolkit). That library will be able to do things like stemming, which removes the conjugated form of verbs so code, codes, coding, coded, coding?, \"code\" are all cod* (and of course it knows to distinguish cod* from the fish \"cod\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        words = i.split()\n",
    "        for j in words:\n",
    "            j = j.lower() # all words are now lower case\n",
    "\n",
    "            try: \n",
    "                if not j[-1].isalpha(): j = j[:-1] # non-alpha suffix\n",
    "                if not j[0].isalpha(): j = j[1:] # non-alpha prefix\n",
    "                if len(j) <= 1: continue # empty strings\n",
    "\n",
    "            except IndexError:\n",
    "                    continue\n",
    "                \n",
    "            # Once cleaned, we can then add the words to a dictionary \n",
    "            # The word will be the 'key' and the frequency will be the 'value'\n",
    "            if j in word_dict: word_dict[j] += 1\n",
    "            else: word_dict[j] = 1\n",
    "\n",
    "print(pd.Series(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(word_dict,name=\"Count\")\n",
    "print(data.value_counts())\n",
    "\n",
    "%pylab inline\n",
    "data.hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Simple regular expressions\n",
    "\n",
    "\n",
    "\"Regular expressions\" are pieces of text that can be expressed in a regular form even if the characters are different. For example, when you encounter a URL on a webpage and right click on it, the browser knows that this is a URL and asks \"open link in new tab\". It does not need to know every URL, just what URLs are supposed to look like (that is they start with \"HTTP://\" or \"HTTPS://\"). \n",
    "\n",
    "This is not the place for a full discussion of the characters used in a regular expression. There exist tons of 'cheat sheets' online. But here are some basics:\n",
    "\n",
    "- []: matches any character in the brackets\n",
    " - [abc] will match either 'a', 'b', or 'c'. [0-9] will match any digit. \n",
    "- \\* matches zero or more characters. \n",
    " - a\\* will match one or more 'a' characters. [\\w]\\* will match zero or more word characters. \n",
    "- ? matches one non-whitespace (or user-defined) character. \n",
    " - a? will match one 'a'. [0-9]? will match one digit.\n",
    "- \\+ matches one or more characters. \n",
    "\n",
    "Below is an exercise to collect numbers from a string of tab-separated text. Follow along carefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "example_text = \"1234\\t3333\\t10000\\t1,500,442\\t3.14\"\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just find the numbers\n",
    "reg01 = re.compile(\"[0-9]\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that it found only single digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just find the numbers using a wildcard\n",
    "reg01 = re.compile(\"[0-9]*\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm...it seems * matches 0 or more instances \n",
    "reg01 = re.compile(\"[0-9]+\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's deal with those punctuation marks\n",
    "reg01 = re.compile(\"[0-9,.]+\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do it with the digit escape code rather than 0-9\n",
    "reg01 = re.compile(\"[\\d,.]+\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above examples, working with regexs involve compiling a 'regular expression' and then applying that to text. Obviously, we could have just split on the tab character in this particular instance, but it's the logic of specifying regexs that's important, such as saying \"all digits\" or \"one or more digits plus a comma.\" In the parentheses for the regular expression we can either ask for 0,1,n or a predetermined number of characters. The characters can be in a range, such as 0-9 or a-z. But we can also use escape codes for the characters. See below for examples of regexs with text.  \n",
    "\n",
    "Lately, I've been using https://regex101.com/ to test regular expressions. It's very handy and thorough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = example_text.replace(\",\",\"\")\n",
    "print(\"Old string: \", example_text)\n",
    "print(\"New string: \", new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"😱 hello everyone, are emojis too basic 😝😝😝 or am I just difficult 🙈? \"\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "reg02 = re.compile(\"[a-z]+\")\n",
    "print(reg02.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "reg02 = re.compile(\"\\S+\")\n",
    "print(reg02.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd \n",
    "\n",
    "example_text = \"😱 hello everyone, are emojis too basic 😝😝😝 or am I just difficult 🙈? \"\n",
    "print(example_text)\n",
    "\n",
    "reg02 = re.compile(\"[😱😝😈🎅🏾🙈]\")\n",
    "emojilist = reg02.findall(example_text)\n",
    "print(emojilist)\n",
    "\n",
    "emojiset = set(emojilist)\n",
    "print(emojiset)\n",
    "\n",
    "print(pd.Series(emojilist).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to the XML above with regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text we can look through it for regularly formatted text. This is ideal for Wikipedia since it is a wiki. Wikis use regularly formatted text for all of its features, and Wikipedians are keen to make sure that the page is formatted properly. It should come as no surprise that MediaWiki itself uses a ton of regular expressions to parse the wiki text in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "infile = open(\"Canada.xml\",'r')\n",
    "\n",
    "wikitext = infile.read()\n",
    "\n",
    "soup = bs4.BeautifulSoup(wikitext, \"lxml\")\n",
    "\n",
    "text_to_parse = soup.mediawiki.page.text\n",
    "# re_inner_links = re.compile(r'\\[\\[.*?\\]\\]')\n",
    "# inner_links = re_inner_links.findall(text_to_parse)\n",
    "\n",
    "re_outer_links = re.compile(r'https?://[\\w\\./?&=%]*')\n",
    "outer_links = re_outer_links.findall(text_to_parse)\n",
    "\n",
    "print(\"The program found %s wikilinks, of which %s are unique.\" % (len(inner_links),len(set(inner_links))))\n",
    "print(pd.Series(outer_links).value_counts()[pd.Series(outer_links).value_counts() > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
