{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 2. Lecture 3. V.1** Author: B. Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2. Day 3. Data formatting, SQL and more text processing\n",
    "\n",
    "Today we will be looking at some further data formatting. We will also be introducing SQL or Structured Query Language. \n",
    "\n",
    "Learning goals: \n",
    "- Understand the use of sql through sqlite. \n",
    "- Parsing Dates \n",
    "- Making use of Map / Apply / Lambda. \n",
    "\n",
    "We will be using some old data related to the 2016 UK election, but it's useful data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Using sql databases\n",
    "\n",
    "SQL stands for Structured Query Language. It is a way to store data in relational tables. This is to say tables that follow the row / column format. Each 'database' can store multiple tables. \n",
    "\n",
    "## Why SQL and not something newer and fancier? \n",
    "There has been a movement in recent years away from relational databases towards newer forms of data stores, such as GraphDB and MongoDB, as part of the 'noSQL' paradigm. First, it's silly to reject SQL as it is pervasive and extremely useful if not in all circumstances. Seccond, it's a little presumptuous to assume that other data stores can do the trick better. Often times there is a temptation to use something else so that we can maintain sloppy data (such as dumping everything into buckets of json) and worry about it later. Well later often comes sooner than you think. It's important to understand SQL first and the later look to see if other solutions make more sense. Here are some SQL alternatives:\n",
    "- **MongoDB** is a store for json-like objects. It's fast, simple and not so powerful except for queuing systems. \n",
    "- **Neo4j** is a 'graph database' that stores entries as nodes and edges. It's hard to wrap your head around, but it does make some form of querying with many joins relatively straightforward where in SQL it might be a real pain. Cases where you want to look at complex relationships that might make joins formidible in SQL can be somewhat easier in neo4j or other graph databases.\n",
    "\n",
    "Having worked with Mongo, Neo4j and SQL, I can confidently say that SQL has a very prominent place in data stores. Lately, a number of domains have started to return to SQL databases. Reddit, for example, switched a few years back from the NoSQL Cassandra to PostGres SQL. \n",
    "\n",
    "Tables in SQL can be **indexed** or not. When they are indexed, using a 'key' it is easier to query and search for data. Keys keep tables organized but are not necessary. A 'primary key' is an index where each entry is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Basics\n",
    "\n",
    "SQL is structured query language, but not a specific database type. Most SQL is interoperable, but some is specific to the type of database. There are also SQL flavours that are used in APIs but restrict certain kinds of queries. For a while Facebook had FQL which allowed developers to query for data on select tables. This has been deprecated in favour of a graph database structure. Some example databases are:\n",
    "- ```Oracle```. The version in use in many industries. \n",
    "- ```MySQL```. An open source SQL implementation in use in a lot of server applications. It is the M in a 'LAMP stack', which stands for Linux, Apache, MySQL and PHP, which is the basis of many dynamic webpage servers. It is being slowly supplanted by NodeJS and its variants such as React, but is still a common way to build and serve dynamic content. \n",
    "- ```PostGRES```. A version that's often used in academia and some other environments. It can be a bit more tricky to set up than MySQL.\n",
    "- ```SQLite```. SQLite databases exist all over your computer, particularly on a mac. This is because you can just read and write to a SQLite database as if it were a file. This is different than the above in that you don't need to set up a server to intereact with the database.  \n",
    "\n",
    "We can use SQLite through a number of libraries. Two popular ones in python are ```sqlalchemy``` and [sqlite3](https://docs.python.org/2/library/sqlite3.html). The former is a little more 'pythonic' in that you can interface with the database through objects and methods. SQLite3 is a little more of a wrapper around direct SQL commands. We will use SQLite3 because it is less abstract as well as allowing you to see SQL commands directly. Below we will see how to open and view details about a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "con = lite.connect('example.db') # This is the 'connection' to the database.\n",
    "\n",
    "cursor = con.cursor() # This is where the program will read or write next in the database.\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE if not exists users(id INTEGER PRIMARY KEY, name TEXT,\n",
    "                       title TEXT, email TEXT unique)\n",
    "''')\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above a couple of important notes. \n",
    "- We use id as an integer primary key. This is automatically generated. \n",
    "- Email is supposed to be unique, if you try to write two users with the same email it will raise an OperationalError. \n",
    "- We 'commit' changes to the database. Databases have connections and cursors. Cursors get things ready, but we actually operate on the database. So notice that it is ```cursor.execute()``` but ```con.commit()```. Typically in very small databases, commiting won't be an issue, but as we start adding much data to the database, if you do not commit this data and then execute another command there is the risk of data loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.cursor()\n",
    "\n",
    "name1 = 'Joss Wright'\n",
    "title1 = 'Senior Research Fellow'\n",
    "email1 = 'joss.wright@oii.ox.ac.uk'\n",
    " \n",
    "name2 = 'Greg Taylor'\n",
    "title2 = 'Associate Professor, Senior Research Fellow, Director of Graduate Studies'\n",
    "email2 = 'greg.taylor@oii.ox.ac.uk'\n",
    "\n",
    "name3 = 'Taha Yasseri'\n",
    "title3 = 'Senior Research Fellow'\n",
    "email3 = 'taha.yasseri@oii.ox.ac.uk'\n",
    "\n",
    "##################################\n",
    "# Three insert statements in a row\n",
    "try: \n",
    "    # 1. \n",
    "    cursor.execute('''INSERT INTO users(name, title, email)\n",
    "                      VALUES(?,?,?)''', (name1,title1, email1))\n",
    "    print('First user inserted')\n",
    "\n",
    "    # 2.\n",
    "    cursor.execute('''INSERT INTO users(name, title, email)\n",
    "                      VALUES(?,?,?)''', (name2, title2, email2))\n",
    "    print('Second user inserted')\n",
    "\n",
    "    # 3.\n",
    "    cursor.execute('''INSERT INTO users(name, title, email)\n",
    "                      VALUES(?,?,?)''', (name3, title3, email3))\n",
    "    print('Third user inserted')\n",
    "\n",
    "    con.commit()\n",
    "except:\n",
    "    print(\"Are you certain the above values are unique?\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import this database into a DataFrame if we want to perform operations in python rather than SQL. We can do this with the pandas.read_sql() command, which will connect to and execute a query on a database in one line. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_sql(\"select * from users\",sqlite3.connect(\"example.db\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above used ``` 'select * from <table>' ``` to get everything in the table. You can be more specific in SQL in a variety of ways. Some relevant ways: \n",
    "- **Where**: This is a clause at the end of the SQL select statement that works as a filter. So you might \n",
    "    - ```select * from users where users.name == \"Joss Wright\"```\n",
    "- **count(column_name)**: This is an aggregation option to count the instances of rows being selected\n",
    "    - ```select count(name) from users```\n",
    "- **Group By**: This is an aggregation option. This is often used to get the data aggregated in subsets. This option with count is similar to value_counts in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from users where title == 'Senior Research Fellow'\",sqlite3.connect(\"example.db\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select count(name) from users\",sqlite3.connect(\"example.db\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select title as Title, count(title) as Count from users group by title\",sqlite3.connect(\"example.db\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL will periodiclly crop up in the course. The logic of SQL is important when in the next lecture we look at merging and grouping. Pandas DataFrames use a logic of merging that is very similar to what you would do in SQL. Further, we will encounter SQL as we look at APIs (Application Programming Interfaces). In many cases APIs also make use of the logic of SQL. Often when using an API you are interfacing with a wrapper that speaks to a database. The wrapper is there so that the user who is receiving the data gets it in a readable form and the data on the server is protected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Dates and the datetime module \n",
    "\n",
    "Dates have a variety of representations. We think of time in a cyclical sense of years, weeks and months. These have meaning for humans. Computers tend to manage time in terms of seconds since 'epoch', which is when Unix time started on January 1, 1970.  \n",
    "\n",
    "Different representations of time can be handled in Python. This week we will be parsing some time data, next week we will use this to plot some times between posting on a website. \n",
    "\n",
    "There are two important modules for time in python. The ```time``` module which handles differences in time, and the management of time. If you want your program to 'sleep' you can run: \n",
    "~~~ python\n",
    "time.sleep( <number_of_seconds> )\n",
    "~~~\n",
    "\n",
    "There are many useful objects under time, but what we probably want is to take dates and manage them. This is covered under the ```datetime``` module. \n",
    "\n",
    "Below we will see how Twitter sends down the datetime in a tweet object. We then have to parse this date if we want to have it in a form that Twitter can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html\n",
    "\n",
    "tweet = {\n",
    " \"created_at\":\"Thu Apr 06 15:24:15 +0000 2017\",\n",
    " \"id\": 850006245121695744,\n",
    " \"id_str\": \"850006245121695744\",\n",
    " \"text\": \"1/ Today we’re sharing our vision for the future of the Twitter API platform!nhttps://t.co/XweGngmxlP\",\n",
    " \"user\": {},  \n",
    " \"entities\": {}\n",
    "}\n",
    "\n",
    "print(type(tweet['created_at']),tweet['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that it is a string, when we really want a date. We can see that the date in this tweet object is represented as: \n",
    "~~~json\n",
    "\"created_at\":\"Thu Apr 06 15:24:15 +0000 2017\",\n",
    "~~~\n",
    "which means:\n",
    "~~~\n",
    "Weekday Month Date 24hour:minute:seconds +timezonehours year\n",
    "~~~\n",
    "\n",
    "To parse this as a datetime object we can use datetime.strptime(). This handy module works similar to a regular expression. You enter in various details about the date in a regular format and it parses these and creates a datetime object. \n",
    "\n",
    "Below first is an example. The escape codes to use can be found in the help file (partially), and more extensively at http://strftime.org/ \n",
    "\n",
    "One thing to note is that by default, it assumes english names for dates. You can set a locale flag for other languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tweetdate = \"time is Thu Apr 06 15:24:15 +0000 2017\"\n",
    "\n",
    "tweetdateobject = datetime.strptime(tweetdate, 'time is %a %b %d %H:%M:%S %z %Y')\n",
    "print(datetime.strftime(tweetdateobject, 'time is %H %H %a %b %d %H:%M:%S %z %Y'))\n",
    "print(type(tweetdateobject),tweetdateobject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have date in a datetime object, you can query basic elements about it as well as represent it as a string and do some time calculations on it. For example, we can as for the date, minute, month, etc... we can also represent the date in Unix time, the Bhuddist calendar or shifted by time zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdo = tweetdateobject\n",
    "print(tdo.date)\n",
    "print(tdo.minute)\n",
    "print(type(tdo.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common thing to do with date time is to tell the difference between two times. This difference is referred to as a ```timedelta```. \n",
    "\n",
    "We can compare the difference between the datetime object we just parsed and another using +/- arithmetic. See the example below. In this example we use 'now', by getting the time of now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now()\n",
    "print(today - tdo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! It seems that unlike the above tweet, 'now' is not timezone aware. We can change this and in doing so make the code run. There are two ways. The first is stripping the timezone off of the tweetdateobject. This is not really ideal. The second is addding the same timezone to now, which is UTC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone \n",
    "\n",
    "######################\n",
    "# The not so good way: \n",
    "tdo_zoneless = tdo.replace(tzinfo=None)\n",
    "now = datetime.now()\n",
    "\n",
    "print(now - tdo_zoneless)\n",
    "\n",
    "##########################\n",
    "# The better way. \n",
    "now = datetime.now(timezone.utc)\n",
    "print(now - tdo)\n",
    "print(type(now - tdo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we got an off-by-one-hour error here. What is that? It's because this course is happening in the UK, which is not running on UTC at the moment. Instead, it is using British Summer Time. As such, we have got an off by one hour error here as the original tweet was coded in UTC and now() is coded in BST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Cleaing up data - exercises in DataFrames\n",
    "\n",
    "Below we will read in an excel table of data. This is a crowdsourced table of data about political candidates. In it we can see the use of ```map```, but not with ```lambda```, instead we simply map the six different gender terms onto a simplified gender binary*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_pol = pd.read_csv(\"PySDS_PolCandidates.csv\",)\n",
    "display(df_pol.head(3))\n",
    "print(df_pol.gender.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...it seems that there's several different ways for gender to have been entered, in addition to the missing data character. To recode gender, we can map values to a new column. Map can either use a function for each cell or a dictionary. The dictionary would take keys as input and the values as output. So if we have six gender categories, those are the input. If we define two output categories ('male' as 0 and '\n",
    "female' as 1) then the dictionary would look thus: \n",
    "\n",
    "~~~ python \n",
    "dict_for_map = { \n",
    "    \"male\":0,\n",
    "    \"Male\":0,\n",
    "    \"Man (sex)\":0,\n",
    "    \"female\":1,\n",
    "    \"Female\":1,\n",
    "    \"Female \":1\n",
    "}\n",
    "~~~\n",
    "\n",
    "Then we map this on to the existing series to create a new column with map. See the full implementation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    \"male\":0,\n",
    "    \"Male\":0,\n",
    "    \"Man (sex)\":0,\n",
    "    \"female\":1,\n",
    "    \"Female\":1,\n",
    "    \"Female \":1\n",
    "}\n",
    "\n",
    "df_pol[\"bgender\"] = df_pol[\"gender\"].map(mapper)\n",
    "df_pol[\"bgender\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that there are 2932 candidates who we identified as male and 1035 identified as female. We use ```dropna=False``` so that we can also see the missing data. In this case, 4 cases did not have their gender identified in the sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make use of this data frame to ask a few questions and then to look at the grouping functions available. First let's ask how many candidates were fielded per party. Then let's ask how many candidates had a twitter handle per party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series 1. How many pepople per party. \n",
    "partyCount = df_pol[\"party\"].value_counts()\n",
    "print(partyCount.head(10))\n",
    "print()\n",
    "\n",
    "# Series 2. How many people per party have a twitter account\n",
    "haveTwitter = df_pol[\"twitter_username\"].notnull()\n",
    "# print(haveTwitter)\n",
    "partyCountWithTwitter = df_pol[haveTwitter][\"party\"].value_counts()\n",
    "# print(partyCountWithTwitter)\n",
    "print(partyCountWithTwitter.tail(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to create a new DataFrame with these two series' that is certainly possible. But remember: \n",
    "\n",
    "~~~ python\n",
    "pd.DataFrame( [  [list1],[list2]  ]  ) \n",
    "~~~\n",
    "\n",
    "will create a dataframe of two rows as long as those lists, not two columns, and we want two columns. So we have a couple options. One worth showing is ```zip()```. Zipping lists come from the way that a zipper works by interleaving the teeth so the two lists:\n",
    "\n",
    "~~~\n",
    "l1 = [1,2,3,4]\n",
    "l2 = [a,b,c,d]\n",
    "new_l = zip(l1,l2)\n",
    "~~~\n",
    "becomes\n",
    "~~~\n",
    "[  [1,a], [2,b], [3,c], [4,d]  ]\n",
    "~~~\n",
    "The problem here is that we don't have two lists, we have two series. They might have different lengths and so forth. We can use these series of different lengths because the DataFrame is created based on the indices. So if there is a 'Conservative' value in series 1 and in series 2 then they will both be aligned in the DataFrame. \n",
    "\n",
    "We can do this in a number of ways. One might be to get all the indices for all the parties, create an empty DataFrame and then merge in the values. But that's a bit overkill. Instead we will do a little shortcut. We will create a DataFrame on the wrong axis by simply including the two series in a collection. Then we will transpose it to get it in the right axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parties = pd.DataFrame([partyCount,partyCountWithTwitter],index=[\"Party Count\",\"Have Twitter\"])\n",
    "display(df_parties.head())\n",
    "\n",
    "df_parties = df_parties.T\n",
    "display(df_parties.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see that above, the first example had the party names as columns. All the data was there but it was in the wrong axis. Then in the second DataFrame, which has been transposed, the data is in the correct layout. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the data\n",
    "\n",
    "Now with this df_parties data properly formatted we can start to ask some questions about the aggregate data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the party candidates have a twitter account? Now it is simply a matter of divding one by the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parties[\"Percent_tweet\"] = \\\n",
    "    df_parties[\"Have Twitter\"] / df_parties[\"Party Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "df_parties[\"Percent_tweet\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what gives with that distribution? It seems pretty skewed to the right. What should we do with that? We can filter the data to get a better look at at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parties[df_parties[\"Party Count\"] > 3][\"Percent_tweet\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring this data will be a part of the assignment. Later we will be exploring how to represent this data in a better format.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
