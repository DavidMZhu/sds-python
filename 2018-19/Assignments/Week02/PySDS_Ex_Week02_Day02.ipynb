{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 02 Day 02 v.1 - Exercise - File Types and Text Processing I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be doing some example regular expressions (yay), and some dataframe manipulation. Recall that we used the Canada wikipedia page as an example. Below is some code that you can use to pull in a Wikipedia page as data. Today, you will be asked to read in several pages, compare them on a number of features in a dataframe and report on what you found.  Below is the code that you can use to download a Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "import bs4 \n",
    "\n",
    "# You can set this Wikipage to be any string that has a wikipedia page.\n",
    "\n",
    "def getWikiPage(page=\"United Kingdom\"): \n",
    "    '''Returns the XML found using special export of a Wikipedia page.'''\n",
    "    \n",
    "    # Here we use urllib.parse.quote to turn spaces and special characters into\n",
    "    # the characters needed for an html string. So for example spaces become %20\n",
    "\n",
    "    URL = \"http://en.wikipedia.org/wiki/Special:Export/%s\" % urllib.parse.quote(WIKIPAGE)\n",
    "\n",
    "    print(URL,\"\\n\")\n",
    "\n",
    "    req = urllib.request.Request( URL, headers={'User-Agent': 'OII SDS class 2018.1/Hogan'})\n",
    "    infile = urllib.request.urlopen(req)\n",
    "\n",
    "    return infile.read()\n",
    "\n",
    "# Testing\n",
    "data = getWikiPage()\n",
    "soup = bs4.BeautifulSoup(wikitext.decode('utf8'), \"lxml\")\n",
    "print(soup.mediawiki.page.revision.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, select 10 countries and place them in a list. \n",
    "# These will be rows in a dataframe. \n",
    "# For each of the ten countries, \n",
    "# find the following features from parsing their wikipedia page: \n",
    "# 1. The number of internal wikilinks. \n",
    "# 2. The number of external wikilinks. \n",
    "# 3. The length of the page (in characters)\n",
    "# 4. The population of the country. \n",
    "#   - This last one will be very tricky. It's okay if you cannot get the \n",
    "#     regex working, or if you have to build multiple regexes. \n",
    "#     Please simply document this. \n",
    "\n",
    "# Print the following: \n",
    "# The rank order of each of the columns. \n",
    "# For example, for wikilinks you might print \n",
    "# (note numbrs below are not accurate)\n",
    "\n",
    "# Table 1. Number of <Wikilinks>\n",
    "# Canada        46\n",
    "# Germany       45\n",
    "# France        24\n",
    "# Netherlands   12\n",
    "# ...\n",
    "\n",
    "# answer below here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reviewer's comments\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
