{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 3 Lecture3. V.1**\n",
    "Last author: B. Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3. Day 3. Pseudocode and Web data access  \n",
    "\n",
    "Learning goals\n",
    "- Understand the function of pseudocode. \n",
    " - Be able to use informal pseduocode\n",
    "- Understand basic text scraping\n",
    "- Understand how to build a basic spider \n",
    "\n",
    "# Section 1. Pseudocode and pseudo-pseudocode. \n",
    "\n",
    "Pseudo code is a means by which we articulate what we wnat to do with code without being too careful syntactically. It's about clearing away the specifics or abstracting them from the code. Often, well written pseudocode can translate very easily into running code. But the purpose is to get a sense of how our code is going to run in general.\n",
    "\n",
    "## Pseduo-pseduocode? \n",
    "\n",
    "Pseudocode is not quite computer code. But it is often written in a format that is close to formal. Have a look at Springer's LINCS books (Lectures in Computer Science) to see that even the pseudocode itself is quite formal. Here is an example below: \n",
    "\n",
    "Now there's no real thing as pseudo-pseudocode. But here I want to suggest that pseduocode varies in its syntactic clarity. More formal pseudocode uses specific mathematical symbols or follows the general syntax of a specific language. More informal pseudoccode is simply a set of instructions, written in an inconsistent or conversational style. This is not a bad thing. The function of pseudocode is to help you organize your thoughts. If you are trying to oragnize and writing it in a certain way helps, then don't fret over its formality. However, when you go to share this with someone else, the more formal, the less likely that there will be ambiguity about what you meant. \n",
    "\n",
    "Below we will have two exercises in pseduocode. The first is I'll give you some code and you write the pseduocode. Then vice versa, I'll give some pseduocode and you write the code. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleaning out the vowels - Pseudocode\n",
    "\n",
    "def vowelSeperator: \n",
    "    for each letter in a string\n",
    "        check if a letter is a vowel\n",
    "        if so then put it in one list\n",
    "        if not put it in a second list\n",
    "    return the two lists  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out the vowels - Working Code: \n",
    "# \n",
    "def vowelSeparator(text):\n",
    "    if not type(text) is str:\n",
    "        return (None,None)\n",
    "    \n",
    "    in_vowels = []\n",
    "    in_consonants = []\n",
    "    \n",
    "    for i in text:\n",
    "        if not i.isalpha():\n",
    "            continue\n",
    "        if i in 'aeiouy':\n",
    "            in_vowels.append(i)\n",
    "        else:\n",
    "            in_consonants.append(i)\n",
    "    return (in_vowels,in_consonants)\n",
    "\n",
    "print( vowelSeparator(\"help\" )  )\n",
    "print( vowelSeparator(\"help %Â£$% erwerwr\" )  )\n",
    "print( vowelSeparator(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fibonacci sequence: Working Code\n",
    "#\n",
    "# A mathematical sequence found in nature, \n",
    "# such as in the shape of a nautilus shell.\n",
    "# See: https://en.wikipedia.org/wiki/Nautilus#Shell\n",
    "\n",
    "def getFibonacci2(n=10):\n",
    "    '''Return the first n digits of the Fibonacci sequence\n",
    "    \n",
    "    Notes: This version uses two counter variables.'''\n",
    "    n1 = 1\n",
    "    n2 = 1\n",
    "    out_list = [n1,n2]\n",
    "    if n <=2:\n",
    "        return out_list\n",
    "    \n",
    "    for i in range(2,n):\n",
    "        out_list.append(n1 + n2)\n",
    "        n2 = n1 + n2\n",
    "        n1 = n2 - n1 # This tricky line is because we have already\n",
    "                     # modifid the value for n2, so to assign n2\n",
    "                     # to n1 we first have to remove the n1 we just\n",
    "                     # added to n2\n",
    "    return out_list\n",
    "\n",
    "def getFibonacci3(n=10):\n",
    "    '''Return the first n digits of the Fibonacci sequence\n",
    "    \n",
    "    Notes: This version uses three counter variables.'''\n",
    "\n",
    "    n1 = 1\n",
    "    n2 = 1\n",
    "    n3 = None\n",
    "    out_list = [n1,n2]\n",
    "    if n <=2:\n",
    "        return out_list\n",
    "    \n",
    "    for i in range(2,n):\n",
    "        n3 = n1 + n2\n",
    "        out_list.append(n3)\n",
    "        n1 = n2  \n",
    "        n2 = n3 \n",
    "        \n",
    "    return out_list\n",
    "\n",
    "\n",
    "getFibonacci3(7)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The fibonacci sequence: Pseudocode\n",
    "#\n",
    "# Don't write every single detail. \n",
    "# First write it in the most basic way possible. \n",
    "# Then consider writing it again with a little more syntax. \n",
    "\n",
    "# Most basic way possible: \n",
    "\n",
    "a ordered sequence of numbers where the third number is the sum of the previous two. \n",
    "we start with 1 and 1 \n",
    "\n",
    "# A little more syntax: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Wikipedia page](https://en.wikipedia.org/wiki/Pseudocode) for some clear examples of pseudocode across languages. In fact, they present the 'fozzie bear' (or in actuality, the 'fizz buzz') program. Notice the comparative pseudocode for this algorithm that takes into account some of the ways that different languages handle the same algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Basic skills for spiders and scrapers\n",
    "\n",
    "One of the goals for today will be to consider web scrapers and web spiders.\n",
    "\n",
    "- **Web scraper**: A program for taking in a page from the web in html and extracting the important details. To scrape a page is often to create meaningful variables in a data structure other than html. Web scraping is a central component of the web. It is how search engines know what is on a page and thus why someone would want to be presented that page as an option in search results.   \n",
    "\n",
    "- **Web spider/web crawler**: A program for navigating the network structure of a part of the web. It begins with a seed set of $n>=1$ pages and then looks for the relevant URLs on the page. For each of the URLs it will download that page and repeat this process. Crawlers range in complexity from the one we will build today to...Google. Indeed, Google started as a crawler that indexed the web, like Hotbot, Lycos, Yahoo and Altavista before it. Interestigly, Google initially succeeded because it took into account the structure of the web as learned from its crawler. This was done through the [page rank algorithm](https://en.wikipedia.org/wiki/PageRank).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 The web scraper\n",
    "\n",
    "Web scrapers are extremely bespoke. In the paper, [Hogan and Berry (2010)](http://comprop.oii.ox.ac.uk/wp-content/uploads/sites/37/2011/06/Hogan-Berry_City_and_Community_Craigslist.pdf) I had to actually rebuild my scraper a few times, as Craigslist would change how the page was laid out, thus necessitating a new scraper. It is worth pointing out that scrapers are among the most fragile parts of data science. Much of the semantics of webpages as seen by the user comes from a combination of layout, order, section and other features that are obvious when viewing the page, but hard to see programmatically. For modern scrapers, they vary from the super basic 'find all the links' variety to ones that include \"<meta>\" tags. \n",
    "\n",
    "There are a variety of webscrapers out there as packages to be used. \n",
    "\n",
    "- ```html```: The basic one would be the python html package, which can read html pages, return tags and hence navigate the html structure. It's like a simplified beautifulSoup.\n",
    "- ```beautifulsoup```: the generic markup parser that we have already seen for xml. In the demo below we will see it in use on html pages. \n",
    "- ```scrapy```: Another python package for scraping pages. I've not used it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example scraping: Twitter Replies\n",
    "\n",
    "Twitter does not easily faciliate the collection of replies of tweets (so far as I know; this information changes quite regularly). But we can get the replies (at least the first set thereof*) from the webpage. Now the twitter webpage is quite busy, so we can use a simpler version of the Twitter page at http://mobile.twitter.com/.  \n",
    "\n",
    "Below is the code to download a twitter mobile page. It will download the page of replies to a tweet I sent. You'll see that this works, but as I walk through the example, it will become clear that this is not always that efficient and it can be very fragile. \n",
    "\n",
    "In class we will discuss how to put this into a series of functions so that we can use it more readily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "username = \"blurky\"\n",
    "tweet_id = 1054644948031692800\n",
    "url = \"https://mobile.twitter.com/%s/status/%s\" % ( username, tweet_id)\n",
    "\n",
    "req = urllib.request.Request( url, headers={'User-Agent': 'OII SDS class 2018.1/Hogan'})\n",
    "infile = urllib.request.urlopen(req)\n",
    "text = infile.read()\n",
    "text = str(text.decode()).replace(\"timeline replies\",\"timeline_replies\")\n",
    "soup = BeautifulSoup(text)#.replace(\"timeline replies\", \"timeline_replies\").encode(\"utf-8\"))\n",
    "\n",
    "reply_text = []\n",
    "reply_ids = []\n",
    "usernames = []\n",
    "\n",
    "x = soup.find_all(class_ =\"timeline_replies\")\n",
    "y = x[0].find_all(class_ =\"tweet-text\")\n",
    "for i in y:\n",
    "    j = i.find_all(class_=\"dir-ltr\")\n",
    "    reply_text.append(j[0].text)\n",
    "    reply_ids.append(i[\"data-id\"])\n",
    "\n",
    "for i in reply_ids: \n",
    "    user_id_re = re.compile(\"href=\\\"/\\w*/reply/%s\" %i)\n",
    "    user_text = user_id_re.findall(text)\n",
    "    if len(user_text) >  0:\n",
    "        usernames.append(user_text[0].split(\"/\")[1])\n",
    "        \n",
    "if len(reply_text) == len(reply_ids) == len(usernames):\n",
    "    reply_df = pd.DataFrame(list(zip(reply_text,reply_ids,usernames)))#,columns=[\"reply_text\",\"tweet_id\",\"username\"])\n",
    "else:\n",
    "    print(\"mismatched series -  check parser\")\n",
    "    \n",
    "reply_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Web Crawlers \n",
    "\n",
    "Web crawlers need to start from somewhere. The least imaginative way would be to do an IP scan. At over 900 trilliion possible IP addresses, many of which are not even available, it might make sense to be a bit more focused.\n",
    "\n",
    "Instead of iterating through IP addresses (which is not even thermodynamically possible with the introduction of IPV6) we should start with a seed set of pages. For each of the page, we would find all the URLs on the page (or at least many of the URLs) and the follow each one in turn. Beyond that we will see in the pseudocode how to do it. Recall that this is going to be pretty basic. To do things a bit more complex you will want to leverage pre-built python packages that abstract some of these details away. Some popular packages are:\n",
    "- ```Selenium```: This is a browser controller. It allows python to click on buttons and controls on firefox as if it was a user. You can interface with a page, log in, get things behind password-protected, submit data on forms, etc...But tweaking this is going to be a real nuisnace. The good news is that it can even execute javascript in a browser (noteable when running SeleniumRC, the 'headless' selenium). \n",
    "- ```Mechanize```: This was originally a module for the ```perl``` language. It was ported over to python over a decade ago. These are maintained separately. Mechanize doesn't interface with a browser so much as set up an instance of a browser in python. It's still in use but not for python 3, so probably not the best to use it here. \n",
    "- ```MechanicalSoup```: As you guessed from beautifulSoup, it is a package for interacting like a browser with pages. It is directly meant to be the successor to Mechanize for Python 3 and uses BeautifulSoup to navigate pages. \n",
    "\n",
    "Bear in mind that these packages are not, properly speaking, webcrawlers. They only facilitate the process of crawling the web. In no case do these packages do the crawling out of the box. So, we will show the crawling and then you can explore this yourself later. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Crawler pseduocode\n",
    "\n",
    "def crawler: \n",
    "    get seed set  \n",
    "\n",
    "    make it list of links_to_visit \n",
    "\n",
    "    while we have more pages and page_count < max_pages\n",
    "           \n",
    "        increment page_counter\n",
    "        scrape page looking for links and stop_word\n",
    "        if we find stop_word in page:\n",
    "            add links to links_to_visit\n",
    "            add page to word_found_list\n",
    "        else:\n",
    "            add page to word_not_found_list\n",
    "\n",
    "    return (word_found_list, word_not_found_list)\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class file: from https://saintlad.com/make-a-web-crawler/\n",
    "# Tweaks by Bernie Hogan\n",
    "# All comments by Jake Kovoor unless prefixed by BH. \n",
    "\n",
    "from html.parser import HTMLParser  \n",
    "from urllib.request import urlopen  \n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "# We are going to create a class called LinkParser that inherits some\n",
    "# methods from HTMLParser which is why it is passed into the definition\n",
    "class LinkParser(HTMLParser):\n",
    "\n",
    "    # This is a function that HTMLParser normally has\n",
    "    # but we are adding some functionality to it\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # We are looking for the begining of a link. Links normally look\n",
    "        # like <a href=\"www.someurl.com\"></a>\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    # We are grabbing the new URL. We are also adding the\n",
    "                    # base URL to it. For example:\n",
    "                    # www.saintlad.com is the base and\n",
    "                    # somepage.html is the new URL (a relative URL)\n",
    "                    #\n",
    "                    # We combine a relative URL with the base URL to create\n",
    "                    # an absolute URL like:\n",
    "                    # www.saintlad.com/somepage.html\n",
    "                    \n",
    "                    newUrl = parse.urljoin(self.baseUrl, value)\n",
    "                    # And add it to our colection of links:\n",
    "                    if \"/study\" in newUrl:\n",
    "                        continue\n",
    "                    self.links = self.links + [newUrl]\n",
    "                    \n",
    "    # This is a new function that we are creating to get links\n",
    "    # that our spider() function will call\n",
    "    def getLinks(self, url):\n",
    "        self.links = []\n",
    "        # Remember the base URL which will be important when creating\n",
    "        # absolute URLs\n",
    "        self.baseUrl = url\n",
    "        # Use the urlopen function from the standard Python 3 library\n",
    "        response = urlopen(url)\n",
    "        # Make sure that we are looking at HTML and not other things that\n",
    "        # are floating around on the internet (such as\n",
    "        # JavaScript files, CSS, or .PDFs for example)\n",
    "        # BH: I changed this to text/html in rather than == text/html, since \n",
    "        #     some pages have text/html; encoding=utf-8.\n",
    "        if 'text/html' in response.getheader('Content-Type'):\n",
    "            htmlBytes = response.read()\n",
    "            # Note that feed() handles Strings well, but not bytes\n",
    "            # (A change from Python 2.x to Python 3.x)\n",
    "            htmlString = htmlBytes.decode(\"utf-8\")\n",
    "            self.feed(htmlString)\n",
    "            return htmlString, self.links\n",
    "        else:\n",
    "            return \"\",[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spider by Bernie Hogan based on pseduocode above\n",
    "# Extra features added to help with crawling \n",
    "# (i.e. pseudocode is not complete)\n",
    "def spider(seed_set,stop_word,max_pages,sleep=0.1): \n",
    "    page_count = 0\n",
    "    pages_with_word = []\n",
    "    pages_without_word = []\n",
    "    all_pages = set(seed_set)\n",
    "    \n",
    "    try: \n",
    "        pages_to_visit  = list(all_pages)\n",
    "    except: \n",
    "        print(\"Spider expects a collection of URLs as first argument.\")\n",
    "        return (None, None)\n",
    "\n",
    "    parser = LinkParser()\n",
    "    \n",
    "    while len(pages_to_visit) and page_count < max_pages:\n",
    "                \n",
    "        url = pages_to_visit[0]\n",
    "        pages_to_visit = pages_to_visit[1:] #Get rid of first page        \n",
    "        page_count += 1\n",
    "\n",
    "        data, links = parser.getLinks(url)\n",
    "\n",
    "        if data.find(stop_word) >-1: #we find word in the data, its position is 0+ \n",
    "            for i in links: \n",
    "                if i not in all_pages:\n",
    "                    all_pages.add(i)\n",
    "                    pages_to_visit.append(i)\n",
    "            pages_with_word.append(url)\n",
    "        else: \n",
    "            pages_without_word.append(url)\n",
    "\n",
    "        time.sleep(sleep)\n",
    "    return (pages_with_word,pages_without_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withwordlist, withoutwordlist = spider([\"https://www.oii.ox.ac.uk/people\"],\"network\",10)\n",
    "print(withwordlist)\n",
    "print(withoutwordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on best practices \n",
    "\n",
    "Search engine crawlers tend to avoid certain areas of a website at the site's request. This is found in a specific file on the site called ```robots.txt```. This file is immediately under the domain name. They vary a lot but tend to ask crawlers not to use the site's search function or scrape private information. Just think of a major site and check it out for yourself! \n",
    "\n",
    "- [Instagram](https://www.instagram.com/robots.txt)\n",
    "- [BBC](https://www.bbc.co.uk/robots.txt)\n",
    "- [Yahoo](https://www.yahoo.com/robots.txt)\n",
    "- [Superbad](http://superbad.com/robots.txt) (My browser start page - it's fun!)\n",
    "- [Reddit](https://www.reddit.com/robots.txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
