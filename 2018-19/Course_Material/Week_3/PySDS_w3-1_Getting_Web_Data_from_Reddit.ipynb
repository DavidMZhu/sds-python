{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 3 Lecture 1. V.1**\n",
    "Last author: B. Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfacing with the web - Part I (Working with Reddit)\n",
    "\n",
    "The World Wide Web is a collection of interlinked resources accessible through URIs. It is a protocol / data layer over the Internet. The documents are a form of hyper-text, meaning that the text itself contains links to other text. \n",
    "\n",
    "Social Data is not necessarily web data or internet data. Yet, the open web has been such a success that interfacing with social data tends to involve some form of online data.\n",
    "\n",
    "For the purposes of the class here, we need to know a few foundational things about the web in order to retrieve data from it programmatically. WE will cover argument strings and then move to an example with authentication in Reddit. This will be a sustained example that will show a variety of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web requests: GET and POST. \n",
    "\n",
    "If you want to get a server to send your computer data , such as a web page, you have to request that data from the server (literally a machine that serves up documents). If the document doesn't care who you are, you can typically use a GET request, since it is simple. You ask for the data at a web address and it returns the data. Now the \"web address\" itself might be a very complex address. But we might still treat it as an address. A POST request, by contrast, sends up data with the address as a 'payload'. \n",
    "\n",
    "Side note: I've enjoyed the presentation of the differences between these two at https://www.diffen.com/difference/GET-vs-POST-HTTP-Requests. But some of those differences are just FYI, but some of them relate to security. The reason we need to know them is because when dealing with APIs, we often have to use the right one or else we cannot authenticate ourselves to a server and thus get the correct data. Below we will use a GET request and then we will skip post in favor of using a 'wrapper'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Requests - Anatomy of a URL \n",
    "\n",
    "A URL (or more generally a URI; Uniform Resource Identifier) is an address used to refer to a specific set of documents or data. We can se URL / URI interchangably, but strictly speaking, URL is a subclass of a URI dedicated to the web. \n",
    "\n",
    "~~~\n",
    "[Transfer  [Domain         [Sub           [File]   [Argument\n",
    "protocol]   name]          domain]                  strings]\n",
    "http://    www.reddit.com /r/learnpython  .json    ?after=t3_a4r234 \n",
    "~~~\n",
    "\n",
    "Notes: \n",
    "- **Transfer Protocol**: How are we transferring the file? Some common protocols: HTTP, HTTPS, FTP, SFTP. All of these tend to run on Internet Protocol addresses (IP addresses). \n",
    "- **Domain name**: The human-readable root page address. This is registered in a naming service so that when you look it up it returns the machine-relevant Internet Protocol address (IP address). Most addresses are IPV4, and take four digits  separated by periods. IPV6 is a very long address designed to ensure there are enough IP addresses for the multitude of possible connections through the 'Internet of Things'. These are your google.com and yandex.ru addresses. \n",
    "- **Top-level Domain**: The part after the last period in a domain name. For BBC.co.uk it is UK.\n",
    "- **Domain name system Servers [DNS servers]**: Look up tables for returning the IP address when given a name. These help translate between the name that you submit for as the URL and the IP address. \n",
    "- **Argument strings**: After the end of a web address you can include arguments. These are formatted as key-value pairs. They are appended to the URL using a ? For example in the URL: \n",
    "~~~\n",
    "https://www.google.com/search?q=argumenttext\n",
    "~~~\n",
    "There is one argument, the key is the letter 'q' (which stands for query) and the value is \"argumenttext\". \n",
    "\n",
    "So a get request will send a URL and some arguents up to a server and receive some data for that. It is what you are probably used to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To employ a URL string there are a couple important features to bear in mind. \n",
    "- Text has to be \"quoted\". So searching for \"First name\" in a browser window actually searches for \"First%20name\".  \n",
    "- The program that sends the request also sends other information along with it including the IP address and the User-Agent string.\n",
    "- **User-agent strings**: Each browser / client (such as a python program ;) has a set user agent string that identifies the client. This is for at least two reasons:\n",
    "  - To see unique computers on a network. \n",
    "  - To format text or the page accordingly. Ever wonder how a page gives you the 'mac' or the 'windows' download? It is because it reads the user-agent string. There are other ways, particularly using javascript to get this information from the user, but the UA is often simple /  good enough. \n",
    "\n",
    "Let's see what the web can learn about your browser. \n",
    "\n",
    "https://www.whoishostingthis.com/tools/user-agent/\n",
    "\n",
    "It learned a lot of information using javascript. To prevent this information  being learned we can surf without it (but we will typically have a bad time without javascript). \n",
    "\n",
    "To work with URLs in python we can use the URL library or ```urllib```. In the example below we will download the OII's mainpage and then print out the first 1024 characters. (Notice what happens when we remove the ```.decode(\"utf-8\")``` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "\n",
    "URL = \"http://www.oii.ox.ac.uk/\"\n",
    "req = urllib.request.Request( URL, headers={'User-Agent': 'OII SDS class 2018.1/Hogan'})\n",
    "infile = urllib.request.urlopen(req)\n",
    "\n",
    "print(infile.read(500).decode(\"utf-8\")) #500 here refers to the first 500 characters. \n",
    "                                        # Printing all of it seems excessive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you recall, we have been looking into using data from the web. We will not be scraping that html page there, but it is certainly possible. We can use a number of strategies from raw downloading with urllib to crawling with pseudo-browsers such as ```Selenium``` and ```Mechanize```. We will be discussing this more later. For now lets look at downloading reddit data in .json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "\n",
    "URL = \"http://www.reddit.com/r/eyebleach.json\"\n",
    "\n",
    "req = urllib.request.Request( URL, headers={'User-Agent': 'OII SDS class 2018.1/Hogan'})\n",
    "\n",
    "infile = urllib.request.urlopen(req)\n",
    "reddit_json = json.loads(infile.read())\n",
    "print(reddit_json.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, reddit loads in as json, where the top element is a dictionry with two keys, 'kind' and 'data'. We can navigate through this dictionry to see what it is we want to parse. Notice that below we can preview the 'after' tag. That tag can help us navigate through the content. This is because reddit ccontent is feed-based, like Twitter, and thus always changing. We can't simply ask for post 1 or 3 because those numbers are only meaningful from a specific time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_json.keys() )\n",
    "print(reddit_json['data'].keys() )\n",
    "print(reddit_json['data']['after'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the argument string.\n",
    "\n",
    "Using Reddit as an example, we can see some of the uses of URL strings for querying dynamic content. In particular, it helps to collect a specific snapshot of a set of content that, typical of social media, is always changing. \n",
    "\n",
    "As Reddit has an infinitely scrolling list of content that changes dynamically, there are a variety of ways to think about collecting this content. The same can be said for tweets or newsfeed items. In reddit's case, the standard is to give you content in their preferred 'top' order. This order is determined by an algorithm that, loosely-speaking, sorts content based on log transformed values of voting over time. But this order then will change dynamically over time. How to page through this document? We can see this in the .json. Surfing to http://api.reddit.com/ you can see that it immediately comes down in json. The root has two keys, 'kind' and 'data'. Data has several keys inside. The one with content is ```children```, but there are two other important keys: ```before``` and ```after```. To page through the document you can use the 'after' key with your url in order to page throught the document. We will demonstrate this below. \n",
    "\n",
    "How would you know this without looking through the JSON file? The Reddit documentation. We should check there now to get a sense of how this should work. We will then implement a way to page through multiple reddit documents, creating a function that helps out. We will also see that reddit has OAuth for authentication. We will examine that after the paging example. \n",
    "\n",
    "The documentation is at https://www.reddit.com/dev/api \n",
    "\n",
    "From there we will also want to look at Reddit's access rules. \n",
    "\n",
    "https://github.com/reddit-archive/reddit/wiki/API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the document said a few things: \n",
    "1. You will be rate limited to 60 queries per minute. So perhaps we should have a ```time.sleep(1)``` method.\n",
    "2. Reddit throttles generic user-agent strings and has a specific formate with which it expects strings. \n",
    "3. When paging you must use the before OR after string, but it is recommended to also include count. \n",
    "    \n",
    "See all three of these put together in the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of import statements here at the top. \n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import urllib.request \n",
    "import pandas as pd\n",
    "\n",
    "import time \n",
    "import json\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have put the bulk of the work in a single function. \n",
    "# it has a few arguments at the top. Now, its also reasonable to split this\n",
    "# into two functions. The first manages the downloads and the second does the \n",
    "# specific downloading \n",
    "def redditReader (page=\"http://api.reddit.com/\",max_items=25,user_agent='',subreddit=None): \n",
    "    ''' Downloading data from reddit\n",
    "    \n",
    "    keyword arguments: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if user_agent == '':\n",
    "        user_agent ={'user-agent':\"'User-Agent': windows:com.demo.sdsclass2018:v1 (by /u/berniehogan)\"}\n",
    "    if subreddit:\n",
    "        page += \"r/%s\" %subreddit\n",
    "    \n",
    "    frame_list = [] #Add each json frame to a list, concat all later.\n",
    "    count = 0\n",
    "    for i in range(math.ceil(max_items / 25)):\n",
    "        print(i)\n",
    "        time.sleep(1)\n",
    "        req = urllib.request.Request( page, headers=user_agent)\n",
    "        infile = urllib.request.urlopen(req)\n",
    "        reddit_json = json.loads(infile.read())\n",
    "        \n",
    "        frame_list.append(json_normalize(reddit_json[\"data\"][\"children\"]))\n",
    "        print(len(frame_list[i]))\n",
    "        page = page.split(\"?\")[0] + \"?after=%s&count=%s\" % (reddit_json[\"data\"][\"after\"],count)\n",
    "        count += 25\n",
    "\n",
    "    return pd.concat(frame_list)\n",
    "\n",
    "result = redditReader(max_items=100,subreddit=\"aww\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes Reddit has stickied posts that stay at the top of the page. We can remove these using the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[result['data.stickied'] != True]\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight diversion: sometimes when concatenating it gets cross when you don't ensure that your columns are in the right order. See the example below for the reasons for the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort alignment warning for future versions: \n",
    "import pandas as pd \n",
    "\n",
    "df1 = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}, columns=['b', 'a'])\n",
    "df2 = pd.DataFrame({\"b\": [5, 6]})\n",
    "\n",
    "print (pd.concat([df1, df2], sort=True),end=\"\\n\\n\")\n",
    "\n",
    "# Notice if you don't sort you have to explicitly say so. In the future, B might get added to the #\n",
    "# bottom of A because it is labelled 'b' but it isn't the right column. \n",
    "print (pd.concat([df1, df2], sort=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result[\"data.title\"].unique())\n",
    "result[\"data.title\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing praw \n",
    "\n",
    "We are going instal an external package now. Why? Because sometimes we want to do something that's not available in Anaconda. In this case, we are going to install the Python Reddit API Wrapper, or praw. This package will manage the act of logging in to reddit so that we can access some of the APIs that we couldn't otherwise. To note, to do this, you will need to have a reddit account. Fortunately, you can create one easily without even an email account. \n",
    "\n",
    "There are a number of ways to install packages and some might be trouble if you don't do it right. We will stick with 'conda', the package manager for anaconda. The second way is using 'pip'. You can do these either through the workbook or the terminal / anaconda prompt. \n",
    "\n",
    "See here for a good summary if how to install and why to be careful:\n",
    "https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "The doucmentation is here:\n",
    "https://praw.readthedocs.io/en/latest/tutorials/comments.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import praw\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install praw\n",
    "    import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAUTH and Keys \n",
    "\n",
    "OAUTH is a form of authentication is used to manage user credentials. We will see this again tomorrow with Twitter, in a slightly different form of OAUTH. In order to get a key we have to first register a reddit name and store a password. In general, OAUTH credentials don't want a password, but this will involve the use of a web request and a third party server. You can see this in the PRAW documentation under [\"Web Application\"](https://praw.readthedocs.io/en/latest/getting_started/authentication.html#web-application). \n",
    "\n",
    "The user name and password aren't, strictly speaking, necessary for credentials, but they are necessary in order to get a series of tokens. Typically OAUTH has public tokens and secret tokens. The public token is used to identify the user, the secret token is used to confirm that the user is who they say they are using 'public key encryption'. It is for 'signing' requests, and this signature is read by the server. https://www.reddit.com/prefs/apps/\n",
    "\n",
    "OAUTH2 is somewhat different in that it does not need the user's credentials at all, but instead gives the user unique access tokens through an intermediary (like a Facebook pop-up dialogue). Let's get our keys and fill them in below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0\n",
    "\n",
    "keys = {\"client_id\":\"\", \n",
    "        \"client_secret\":\"\", \n",
    "        \"username\":\"berniehogan\", \n",
    "        \"password\":\"\"}\n",
    "\n",
    "with open(\"keys.json\",'w') as infile:\n",
    "    infile.write( json.dumps(keys) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have filled in those values, cleared the 1/0 and run the code, you should have all the details stored in a .json file in the same directory as your code. Now, you can delete the cell above and simply read the keys.json file as needed. Below, we will load in the keys file and then use it to create a ```reddit``` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = json.loads(open(\"keys.json\").read())\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction (by /u/berniehogan)',\n",
    "                     client_id=keys['client_id'], \n",
    "                     client_secret=keys['client_secret'],\n",
    "                     username=keys['username'], \n",
    "                     password=keys['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that it worked. \n",
    "# If it did it will print my name.\n",
    "# If it didn't it will throw an error. \n",
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed that we are indeed logged in and have a user account token through OAuth, we can start to interact with the reddit API as a series of python objects, rather than simply as a set of unstructured .json. Now note that this is very tricky, as sometimes data comes down in ways you might not expect. It is good practice to check for errors, test your code and read the documenntation. \n",
    "\n",
    "## Analysing a reddit AMA: Frank OZ of the muppets\n",
    "\n",
    "Approxmately nine months ago Frank Oz, the puppeteer behind Fozzie Bear and  Miss Piggy did an AMA on Reddit. AMAs are \"Ask Me Anything\", and often hosted by the reddit administration themselves. To query for the reddit submission, we can create a ```submission``` object. These can be created using the full URL that can be found in the browser, or mire lightweight - the Reddit ID for the content. These IDs are categorically tagged sequential objects. The categories are \"t1\" through \"t6\", \n",
    "- t1_\tComment\n",
    "- t2_\tAccount\n",
    "- t3_\tLink\n",
    "- t4_\tMessage\n",
    "- t5_\tSubreddit\n",
    "- t6_\tAward\n",
    "\n",
    "A submission is a t3 object, whereas a user is a t2 object. Be careful, because sometimes the object has the category attached. It's not clear when it does or doesn't have this prefix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subr = reddit.subreddit('aww')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = reddit.subreddit('nba')\n",
    "posts = nba.hot(limit=200)\n",
    "for i in posts: \n",
    "    print(i.title)\n",
    "    dir(i)\n",
    "#     print(i.authorname)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect a submission \n",
    "submission = reddit.submission(id='7o6bxv')\n",
    "for i in dir(submission): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submisssion has a great deal of metadata in there. This is different from the data that we got from the link on the submission page (seen in the DataFrame above). These objects are not all static names. Some of them are nested objects in their own right. Each submission has a number of comments, each comment has an author, each author has their own details. This can all be downloaded by navigating the object. \n",
    "\n",
    "We will be looking at comments below. By getting the first comment, we are not querying the full comment tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = submission.comments.list()[0]\n",
    "print(dir(comment))\n",
    "print()\n",
    "print(dir(comment.author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's a lot of info for both the comment and the author. We are going to build a small dataframe for all the comments in this submission. Then we can ask some questions of the comments. One of these questions will be done with just this dataframe, but one of them will require us to build a tree structure to represent the comment tree. \n",
    "```\n",
    "<submission> \n",
    "    └<root_comment>\n",
    "        └<child_comment>\n",
    "            └<child_comment>\n",
    "    └<root_comment>\n",
    "        └<child_comment>\n",
    "        └<child_comment>\n",
    "            └<child_comment>\n",
    "        └<child_comment>\n",
    "```\n",
    "This comment tree is known as a DAG, a directed, acyclic graph. The edges are from the children comments to the parent comment. \n",
    "\n",
    "We can use the DAG to ask some questions about the overall structure of the comment tree. We don't need to ask about the whole tree, but it is interesting as a means for understanding the dynamics on reddit. This follows the work of https://www.ingentaconnect.com/content/pal/paljit/2010/00000025/00000002/art00011\n",
    "\n",
    "who classified forums on slashdot according to their thread depth and thread width. Shallow, wide threads were associated with gaming and genaral leisure. Long, narrow threads were associated with politics. Yet, interesting operating systems had a structural trace that was similar. Back in 2010 it seemed Mac vs. Windows was in full force. \n",
    "\n",
    "Before building a tree, however, we should first build a DataFrame of all the comments. We can then use this DataFrame to build a DAG. What should we put in our DataFrame? I've inspected several elements and I think the ones below are interesting: \n",
    "(note that author.name might be missing, as is the case with deleted content) This might spell trouble later on when comment.author.name returns None. Consequently, none is replaced with ```[deleted]```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comment.body)\n",
    "print(comment.ups)\n",
    "print(comment.author.name)\n",
    "print(comment.created_utc)\n",
    "print(comment.depth)\n",
    "print(comment.is_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will get all the comments. This is part of PRAW. With ```limit=none``` it will go through all the comment tree, one at a time, while observing the API query limits (i.e. one query per second), so it might take a couple minutes to get all the comments for this particular thread.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "print(\"Got all the comments\")\n",
    "print(len( submission.comments.list()  )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them all into a data frame. Not all the data, mind you, but the data that we think is interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "flag = True \n",
    "for c,comment in enumerate(submission.comments.list()):\n",
    "    # THe try exists because some reddit comments are from authors who \n",
    "    # have deleted their account, but the comments persist. \n",
    "    try:\n",
    "        x = comment.author.name,\n",
    "        authorname = x[0]\n",
    "    except AttributeError:\n",
    "        authorname = \"[deleted]\"\n",
    "\n",
    "    try:\n",
    "        comment_body = comment.body,\n",
    "    except AttributeError:\n",
    "        comment_body = \"[deleted]\"\n",
    "\n",
    "    try: \n",
    "        df_list.append([ \\\n",
    "        comment.id,\n",
    "        comment_body,\n",
    "        authorname,\n",
    "        comment.ups,\n",
    "        comment.created_utc,\n",
    "        comment.depth,\n",
    "        comment.parent_id[3:]\n",
    "        ])\n",
    "    except AttributeError:\n",
    "        if flag: \n",
    "            print(\"Are you sure you collected all the comments first?\")\n",
    "            flag = False\n",
    "        \n",
    "reddit_df = pd.DataFrame(df_list,columns=[\"id\",\"body\",\"authorname\",\"ups\",\"created_utc\",\"depth\",\"parent_id\"])\n",
    "display(reddit_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just with the comment data, we can do some interesting things. For exampple, we can convert the UTC timestap to a datetime object in the DataFrame and then plot the upvotes overtime. Again, back to map and lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "reddit_df[\"utc\"] = reddit_df[\"created_utc\"].map(lambda x: datetime.utcfromtimestamp(x) )\n",
    "reddit_df[\"zero_utc\"] = reddit_df[\"created_utc\"] - reddit_df[\"created_utc\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "reddit_df.plot.scatter(\"zero_utc\",\"ups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DAG. \n",
    "\n",
    "This is not a course in network analysis, but it does not need to be to get some of the very basics down. First, a network or graph has nodes (in this case comments) and edges (in this case, the 'in reply to' structure). A reply to a reply to a reply has a depth of 3.\n",
    "- What is the deepest comment thread? \n",
    "- What comment prompted the most replies? \n",
    "\n",
    "First we build a network, then we take some measurements of the network, then we can merge this back with the main data frame and start looking at some of the properties of the comments. Later you might wish to ask questions such as, do wide comments get more upvotes than narrow ones? Is there any difference between comments based on sentiment? What does the largest comment tree look like? \n",
    "\n",
    "First we import ```networkx```, one of the two leading python network programs. The other ```igraph``` is  real nuisance to install and the graphic package for it is downright formidable to install. It has nice clustering features though, but that's for another course. \n",
    "\n",
    "Second, we go through the nodes twice, the first time creating a new node for each comment in the file. The second time, we add the links between all of them. You could do this in one go, but the issue is that we wanted to add the ```name``` and the ```depth``` to the node before building the links. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import pandas as pd\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "node_set = set([])\n",
    "# submission.comments.replace_more(limit=None)\n",
    "for i in submission.comments.list():\n",
    "    try:\n",
    "        G.add_node(i.id, depth=i.depth,name=i.author.name)\n",
    "    except AttributeError: \n",
    "        G.add_node(i.id, depth=i.depth,name=\"[Deleted]\")\n",
    "    node_set.add(i.id)\n",
    "\n",
    "for i in submission.comments.list():\n",
    "    if i.parent_id and i.parent_id[3:] in node_set:\n",
    "        G.add_edge(i.id,i.parent_id[3:])\n",
    "\n",
    "# pd.Series([G.subgraph(c).number_of_nodes() for c in nx.connected_components(G)]).plot('hist')\n",
    "\n",
    "print(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laying out the graph \n",
    "\n",
    "This is some boilerplate code and more for illustrative purposes. As you can see, it does some pretty standard steps - layout the nodes according to an algorithm, draw nodes, draw edges, display plot. In this case, it is the Kamada Kawai algorithm for layout. There are others that might work better but this one is available. The only thing that might not be obvious is the 'weakly_connected_component_subgraphs' method. This method looks for the connected subgraphs. Each root comment is its own subgraph that includes all of the root node's children. We want to have a look at the largest one of these. So we take the 'max' of the set of the weakly connected components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# # Determine the layout.\n",
    "Gc = max(nx.weakly_connected_component_subgraphs(G), key=len)\n",
    "pos=nx.kamada_kawai_layout(Gc)\n",
    "\n",
    "# Drawing the nodes, and setcting the size. \n",
    "nx.draw_networkx_nodes(Gc,pos,node_size=10)\n",
    "\n",
    "# Drawing the edges, and setting the width\n",
    "nx.draw_networkx_edges(Gc,pos)\n",
    "\n",
    "# Drawing some labels \n",
    "# nx.draw_networkx_labels(g, pos)\n",
    "\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below here we are using the concept of \"in-degree\", which is just a count of the number of edges that point to a node. A high in-degree means lots of replies. Does a root node get the most replies? Turns out no, it is a comment of depth 2 that gets a whopping 29 replies, far outpacing the next highest.\n",
    "\n",
    "The code below is a bit denser than usual, perhaps. We can unpack it by taking little bits of it at a time and learning what it does. For example, we notice that G.in_degree doesn't return a dict, but a dict-like object that we must turn into a dictioary. Then we can group by depth. Here instead of taking the ```mean``` or the ```count``` as is often the case, we will take the maximum in-degree. However, two nodes at the same depth might both have the same indegree. We take both of them, then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i,j in G.in_degree():\n",
    "    print(i,j)\n",
    "    count += 1\n",
    "    if count > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a comment ID as the ID of the node, and the in degree as the value. Thus we can merge this back into our original DataFrame now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_dict = pd.DataFrame.from_dict (  dict(G.in_degree()), orient=\"index\",columns=[\"in_degree\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_dict['depth'] = pd.Series(  \n",
    "    dict([ (i,G.nodes[i]['depth']) for i in G.nodes] ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_list = []\n",
    "for i,j in deg_dict.groupby(\"depth\"):\n",
    "    df_list.append(j[j[\"in_degree\"] == max(j[\"in_degree\"])].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_replies_per_depth = pd.concat(df_list)\n",
    "display(max_replies_per_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the comments, we can merge them back into the main DataFrame and then look at the longest comments. In this case, I made a new, smaller dataframe with just the comments that qualified as the widest at each depth. We can see the one that had 29 replies. It is, interestingly, not from Fzank Oz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = max_replies_per_depth.merge(reddit_df,left_index=True, right_on=\"id\")\n",
    "display(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['body'] = z['body'].map(lambda max_replies_per_depth:max_replies_per_depth[0])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.loc[616,\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for today, we can read the longest thread in the entire comment set as its own by using networkx to query for the longest path and then uerying the DataFrame for those comments. In theory, there should really only be one entry at each depth, so we can sort by depth and then print out the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_path_comment_branch = list(nx.dag_longest_path(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_path_comment_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpcb_df =reddit_df[reddit_df[\"id\"].isin(longest_path_comment_branch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpcb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sorted = lpcb_df.sort_values(\"depth\",ascending=True)\n",
    "\n",
    "for index,row in temp_sorted.iterrows():\n",
    "    print(\"%s said:\\n%s\\n---\\n\" % (row.authorname,row.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
