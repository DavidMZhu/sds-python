{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 03 Day 02 v.1 - Exercise - Merging and reporting on data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Create a new codebook\n",
    "\n",
    "For this exercise, please go through all the steps in class with respect to cleaning the data on roottweets, except do this for the replytweets table. Put this in a function that you can call. It is okay if the function is very specific to replytweets, but the more generic the better. If it could also be used to clean up roottweets from a raw SQL call, this would be ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1. \n",
    "###############################################\n",
    "# Answer below here \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Reviewer comments below here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Finding the happy tweets. \n",
    "\n",
    "Using the indepedent samples ttest function, split the data into at least two groups (e.g., by length of tweets / has emoji / has @mention, etc...). Compare two of theese groups using an independent samples t-test. (See example below). Try to find a split that will lead to a significant difference between the two splits. After trying three different splits, if there is no significant difference, simply move on. Report all three splits. If you get a significant difference on the first split, great! This can be done with either the roottweets table or the replytweets table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Ttest_indResult(statistic=0.7662141663581955, pvalue=0.4583568003180871)\n",
      "0.0493953176745734\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Example ttest code \n",
    "from scipy import stats\n",
    "%pylab inline\n",
    "\n",
    "r1 = [1,3,5,7,9,11]\n",
    "r2 = [1,3,4,6,8,3,6,7]\n",
    "r3 = [80,10,20,31,4,45]\n",
    " for paired samples it's ttest_rel(x,y)\n",
    "print(stats.ttest_ind(r1,r3).pvalue)\n",
    "\n",
    "################################\n",
    "# Answer below here \n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# Peer review comments below here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Finding the tweetstorm tweet. \n",
    "\n",
    "We want to find out what tweet inspired the most negative replies. First, create a 'grouped_reply_tweets' table/DataFrame.  It should have the roottweet_id, the count of replies, and the average sentiment score for pos, neg, neu. \n",
    "\n",
    "Filter this table to those roottweets that have > 1 replies. Look for the tweet(s) with the maximum average negative sentiment. If there are more than one with the same max negative sentiment, take the roottweet(s) with the most replies. Use these tweet IDs to look up the tweet(s) in the roottweets table. What tweet was it that prompted such negativity? Report your output as follows: \n",
    "\n",
    "```\n",
    "The maximum negative sentiment score was %s. The replies that got this score were:\n",
    "\n",
    "Tweet 1.\n",
    "<tweet> \n",
    "\n",
    "Tweet 2. \n",
    "<tweet>\n",
    "\n",
    "etc...\n",
    "\n",
    "The root tweet that inspired such negativity was written by @<user>. It was: \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Answer below here \n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# Peer review comments below here \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
